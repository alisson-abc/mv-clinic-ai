# An√°lise do STT (Speech-to-Text) da Sofya

## üìã Resumo Executivo

O STT da Sofya √© implementado usando **Dialogflow CX Speech-to-Text** no backend (`chat-marketplace-back`). O processo funciona via WebSocket em tempo real, onde o √°udio √© enviado em chunks, processado pelo Dialogflow CX, e a transcri√ß√£o √© retornada ao cliente.

---

## üèóÔ∏è Arquitetura do STT

### Fluxo de Dados

```
Cliente (Frontend)
    ‚Üì
[Captura √Åudio PCM] ‚Üí [Envia chunks via WebSocket] ‚Üí [Backend recebe]
    ‚Üì
[Backend acumula no buffer] ‚Üí [Usu√°rio para de falar] ‚Üí [Processa stream]
    ‚Üì
[Dialogflow CX STT] ‚Üí [Retorna transcri√ß√£o] ‚Üí [Envia via WebSocket]
    ‚Üì
[Cliente recebe transcri√ß√£o]
```

### Componentes Principais

1. **Frontend** (`chat-marketplace-front` ou `mv-clinic-ai`)
   - Captura √°udio do microfone
   - Converte para PCM 16-bit
   - Envia chunks via WebSocket

2. **Backend** (`chat-marketplace-back`)
   - Recebe chunks de √°udio
   - Acumula no buffer
   - Processa com Dialogflow CX quando usu√°rio para de falar

3. **Dialogflow CX** (Google Cloud)
   - Faz STT (Speech-to-Text)
   - Retorna texto transcrito
   - Detecta inten√ß√µes

---

## üìÅ Arquivos Relevantes

### Backend (`chat-marketplace-back`)

#### 1. `websocket_handler.py`
**Responsabilidade:** Gerencia conex√µes WebSocket e processa √°udio

**Fluxo STT:**
```python
# 1. Recebe chunks de √°udio
async def handle_audio_chunk(self, audio_base64: str):
    audio_bytes = self.audio_processor.base64_to_bytes(audio_base64)
    self.audio_buffer.append(audio_bytes)  # Acumula no buffer

# 2. Quando usu√°rio para de falar
async def handle_stop_speaking(self):
    if len(self.audio_buffer) > 0:
        await self.process_audio_stream()  # Processa buffer

# 3. Processa stream com Dialogflow
async def _process_dialogflow_stream(self, audio_chunks):
    async for response in self.dialogflow.streaming_detect_intent(...):
        # 4. Extrai transcri√ß√£o da resposta
        if 'text' in response:
            transcription_msg = TranscriptionMessage(
                type=MessageType.TRANSCRIPTION,
                session_id=self.session_id,
                data={'text': response['text']}
            )
            await self.send_message(transcription_msg)  # Envia ao cliente
```

**Pontos Importantes:**
- Buffer de √°udio: `deque(maxlen=100)` - armazena at√© 100 chunks
- VAD (Voice Activity Detection): Detecta quando usu√°rio come√ßa/para de falar
- Barge-in: Pode interromper processamento se usu√°rio come√ßar a falar novamente

#### 2. `dialogflow_service.py`
**Responsabilidade:** Integra√ß√£o com Dialogflow CX para STT

**M√©todo Principal:**
```python
async def streaming_detect_intent(
    self,
    session_id: str,
    audio_chunks: AsyncIterator[bytes],
    sample_rate: int = 16000
) -> AsyncIterator[Dict[str, Any]]:
```

**Configura√ß√£o de √Åudio:**
```python
audio_config = InputAudioConfig(
    audio_encoding=AudioEncoding.AUDIO_ENCODING_LINEAR_16,  # PCM 16-bit
    sample_rate_hertz=sample_rate,  # 16000 ou 24000 Hz
    single_utterance=False,  # Permite m√∫ltiplas intera√ß√µes
)
```

**Processamento:**
1. Coleta todos os chunks do buffer
2. Cria stream de requisi√ß√µes para Dialogflow CX
3. Envia requisi√ß√£o inicial + chunks de √°udio
4. Recebe respostas do Dialogflow
5. Extrai texto transcrito de `response.query_result.response_messages`

**Resposta do Dialogflow:**
```python
# Estrutura da resposta
{
    'text': 'Texto transcrito da fala do usu√°rio',
    'intent': {
        'name': 'Nome da inten√ß√£o',
        'confidence': 0.95
    },
    'parameters': {...},  # Par√¢metros extra√≠dos
    'payload': {...}  # Chamadas de ferramentas
}
```

#### 3. `audio_processor.py`
**Responsabilidade:** Convers√£o e processamento de √°udio

**Fun√ß√µes Principais:**
- `base64_to_bytes()`: Converte Base64 ‚Üí bytes
- `bytes_to_base64()`: Converte bytes ‚Üí Base64
- `bytes_to_numpy()`: Converte bytes PCM ‚Üí numpy array
- `resample_audio()`: Reamostra √°udio para diferentes sample rates
- `normalize_audio()`: Normaliza √°udio para evitar clipping

#### 4. `models/messages.py`
**Responsabilidade:** Define estrutura de mensagens WebSocket

**Mensagem de Transcri√ß√£o:**
```python
class TranscriptionMessage(ServerMessage):
    type: Literal[MessageType.TRANSCRIPTION] = MessageType.TRANSCRIPTION
    data: Dict[str, str]  # {'text': 'Texto transcrito'}
```

**Formato JSON enviado:**
```json
{
  "type": "transcription",
  "session_id": "uuid-da-sessao",
  "data": {
    "text": "Texto transcrito da fala do usu√°rio"
  },
  "timestamp": 1234567890.123
}
```

---

## üîß Configura√ß√£o do STT

### Par√¢metros Importantes

#### Sample Rate
- **Padr√£o:** 16000 Hz (Dialogflow CX)
- **Configur√°vel:** Via `settings.sample_rate`
- **Suportado:** 16000, 24000 Hz

#### Formato de √Åudio
- **Encoding:** LINEAR16 (PCM 16-bit)
- **Canais:** Mono (1 canal)
- **Formato de envio:** Base64

#### Buffer
- **Tamanho m√°ximo:** 100 chunks
- **Tipo:** `deque` (FIFO - First In, First Out)
- **Limpeza:** Ap√≥s processamento bem-sucedido

---

## üì° Protocolo WebSocket

### Mensagens do Cliente ‚Üí Servidor

#### 1. Chunk de √Åudio
```json
{
  "type": "audio_chunk",
  "session_id": "uuid",
  "data": {
    "audio": "base64_encoded_audio_chunk"
  }
}
```

#### 2. Usu√°rio Parou de Falar
```json
{
  "type": "stop_speaking",
  "session_id": "uuid"
}
```

ou

```json
{
  "type": "end_of_speech",
  "session_id": "uuid"
}
```

### Mensagens do Servidor ‚Üí Cliente

#### 1. Transcri√ß√£o
```json
{
  "type": "transcription",
  "session_id": "uuid",
  "data": {
    "text": "Texto transcrito"
  },
  "timestamp": 1234567890.123
}
```

#### 2. Inten√ß√£o Detectada
```json
{
  "type": "intent",
  "session_id": "uuid",
  "data": {
    "intent": {
      "name": "Nome da inten√ß√£o",
      "confidence": 0.95
    }
  }
}
```

---

## üéØ Estrat√©gias de IA Suportadas

O backend suporta m√∫ltiplas estrat√©gias via par√¢metro `strategy` na URL do WebSocket:

1. **`dialogflow`** - Dialogflow CX (padr√£o)
2. **`notebookmv`** - Notebook MV (RAG)
3. **`sofya`** - Sofya LLM

**Exemplo:**
```
wss://chat-marketplace-back-335214030459.us-central1.run.app/ws/voice-chat?strategy=sofya&language=pt-BR
```

---

## üîç Detalhes T√©cnicos

### Processamento de Stream

1. **Acumula√ß√£o:**
   - Chunks s√£o acumulados no buffer `audio_buffer`
   - Buffer √© um `deque` com tamanho m√°ximo de 100

2. **Trigger de Processamento:**
   - Quando usu√°rio para de falar (`stop_speaking`)
   - Buffer √© processado como stream completo

3. **Streaming para Dialogflow:**
   - Requisi√ß√£o inicial com configura√ß√£o de √°udio
   - Chunks subsequentes com dados de √°udio
   - Cada chunk precisa incluir `language_code`

4. **Respostas:**
   - Dialogflow retorna respostas incrementais
   - Cada resposta pode conter:
     - Texto transcrito (`text`)
     - Inten√ß√£o detectada (`intent`)
     - Par√¢metros extra√≠dos (`parameters`)
     - Chamadas de ferramentas (`payload`)

### VAD (Voice Activity Detection)

O backend tamb√©m tem VAD pr√≥prio:
- **Arquivo:** `vad_service.py`
- **Uso:** Detecta se h√° fala no chunk de √°udio
- **Fun√ß√£o:** `is_speech(audio_bytes)` retorna `True/False`

**Nota:** O VAD do backend √© usado para detectar in√≠cio/fim de fala, mas o STT real √© feito pelo Dialogflow CX.

---

## üêõ Poss√≠veis Problemas e Solu√ß√µes

### 1. Transcri√ß√£o n√£o aparece
**Causa:** Buffer vazio ou Dialogflow n√£o retornou texto
**Solu√ß√£o:** Verificar se chunks est√£o sendo enviados e se Dialogflow est√° configurado corretamente

### 2. Transcri√ß√£o parcial
**Causa:** Stream foi interrompido (barge-in ou timeout)
**Solu√ß√£o:** Verificar logs do backend para ver se stream foi cancelado

### 3. Erro de encoding
**Causa:** Formato de √°udio incorreto
**Solu√ß√£o:** Garantir que √°udio est√° em PCM 16-bit, mono, 16kHz ou 24kHz

### 4. Lat√™ncia alta
**Causa:** Muitos chunks no buffer ou rede lenta
**Solu√ß√£o:** Otimizar tamanho dos chunks ou reduzir frequ√™ncia de envio

---

## üìä Logs e Debug

### Backend Logs
```python
# Quando usu√°rio come√ßa a falar
logger.info(f"Sess√£o {session_id}: Usu√°rio come√ßou a falar")

# Quando processa stream
logger.debug("Iniciando streaming_detect_intent")
logger.debug(f"Enviando {len(audio_chunks_list)} chunks de √°udio")

# Quando recebe transcri√ß√£o
logger.debug(f"Resposta recebida: {type(response)}")
```

### Verificar no Frontend
```javascript
// Log quando recebe transcri√ß√£o
ws.onmessage = (event) => {
  const data = JSON.parse(event.data);
  if (data.type === 'transcription') {
    console.log('üìù Transcri√ß√£o recebida:', data.data.text);
  }
};
```

---

## üîÑ Compara√ß√£o: Frontend vs Backend

### Frontend (`mv-clinic-ai`)
- **VAD:** Detecta in√≠cio/fim de fala (para UI)
- **STT:** N√£o faz STT diretamente, apenas envia √°udio
- **Recebe:** Transcri√ß√£o do backend via WebSocket

### Backend (`chat-marketplace-back`)
- **VAD:** Detecta fala nos chunks (para barge-in)
- **STT:** Faz STT usando Dialogflow CX
- **Envia:** Transcri√ß√£o para o frontend via WebSocket

---

## üìù Conclus√£o

O STT da Sofya funciona da seguinte forma:

1. **Frontend** captura √°udio e envia chunks via WebSocket
2. **Backend** acumula chunks no buffer
3. Quando usu√°rio para de falar, **backend** processa buffer
4. **Dialogflow CX** faz STT e retorna texto transcrito
5. **Backend** envia transcri√ß√£o de volta via WebSocket
6. **Frontend** recebe e exibe transcri√ß√£o

**Tecnologia:** Dialogflow CX Speech-to-Text (Google Cloud)
**Formato:** PCM 16-bit, Mono, 16kHz ou 24kHz
**Protocolo:** WebSocket (streaming em tempo real)

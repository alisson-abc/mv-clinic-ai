/**
 * Hook React para Chat por Voz (Orb de Voz)
 * Integra VAD, captura de √°udio PCM, reprodu√ß√£o e WebSocket
 * Baseado na implementa√ß√£o do chat-marketplace-front
 */

import { useState, useEffect, useRef, useCallback } from 'react';
import { VoiceChatService, VoiceChatConfig } from '@/services/voiceChat';
import { VadService } from '@/services/vad';
import { AudioCaptureService } from '@/services/audioCapture';
import { AudioPlaybackService } from '@/services/audioPlayback';
import { VAD_CONFIG } from '@/services/config';

export interface VoiceChatMessage {
  id: string;
  text: string;
  type: 'user' | 'assistant';
  timestamp: Date;
}

export interface UseVoiceChatReturn {
  isConnected: boolean;
  isRecording: boolean;
  isListening: boolean; // VAD ativo
  isSpeaking: boolean; // Bot est√° falando
  transcription: string;
  messages: VoiceChatMessage[]; // Mensagens formatadas
  connect: () => Promise<void>;
  disconnect: () => void;
  startVoiceMode: () => Promise<void>;
  stopVoiceMode: () => void;
  sendText: (text: string) => void;
  error: Error | null;
}

export function useVoiceChat(
  config?: VoiceChatConfig
): UseVoiceChatReturn {
  const [isConnected, setIsConnected] = useState(false);
  const [isRecording, setIsRecording] = useState(false);
  const [isListening, setIsListening] = useState(false);
  const [isSpeaking, setIsSpeaking] = useState(false);
  const [transcription, setTranscription] = useState('');
  const [messages, setMessages] = useState<VoiceChatMessage[]>([]);
  const [error, setError] = useState<Error | null>(null);
  const [userIsSpeaking, setUserIsSpeaking] = useState(false); // Flag para detectar se usu√°rio est√° falando AGORA

  // Refs para servi√ßos
  const serviceRef = useRef<VoiceChatService | null>(null);
  const vadServiceRef = useRef<VadService | null>(null);
  const audioCaptureRef = useRef<AudioCaptureService | null>(null);
  const audioPlaybackRef = useRef<AudioPlaybackService | null>(null);
  
  // Refs para estado (para usar em callbacks)
  const transcriptionRef = useRef<string>('');
  const messagesRef = useRef<VoiceChatMessage[]>([]);
  const userIsSpeakingRef = useRef<boolean>(false);
  
  // Sincronizar refs com estado
  useEffect(() => {
    transcriptionRef.current = transcription;
  }, [transcription]);
  
  useEffect(() => {
    messagesRef.current = messages;
  }, [messages]);

  /**
   * Handler: Barge-in (interrup√ß√£o)
   */
  const handleBargeIn = useCallback(() => {
    if (audioPlaybackRef.current) {
      audioPlaybackRef.current.stopPlayback();
    }
    if (serviceRef.current) {
      serviceRef.current.clearBuffer();
    }
    setIsSpeaking(false);
  }, []);

  /**
   * Conecta ao WebSocket
   */
  const connect = useCallback(async () => {
    // Evitar m√∫ltiplas conex√µes
    if (serviceRef.current?.isConnected()) {
      console.log('‚ÑπÔ∏è WebSocket j√° est√° conectado, ignorando nova conex√£o');
      return;
    }
    
    try {
      console.log('üîå Conectando ao WebSocket:', config);
      const service = new VoiceChatService(
        undefined, // onMessage n√£o usado mais
        (err) => {
          console.error('‚ùå Erro WebSocket:', err);
          setError(err);
          setIsConnected(false);
        },
        () => {
          console.log('üîå WebSocket desconectado');
          setIsConnected(false);
        },
        config
      );

      // Configurar callbacks de √°udio e texto
      service.setOnAudio((audioData: ArrayBuffer) => {
        console.log('üîä √Åudio recebido do servidor:', audioData.byteLength, 'bytes');
        
        // Barge-in DESABILITADO para POC - sempre reproduz o √°udio
        // TODO: Reativar Barge-in quando necess√°rio
        // if (userIsSpeakingRef.current) {
        //   console.log('üõë Barge-in: usu√°rio est√° falando, parando reprodu√ß√£o');
        //   if (audioPlaybackRef.current) {
        //     audioPlaybackRef.current.stopPlayback();
        //   }
        //   if (serviceRef.current) {
        //     serviceRef.current.clearBuffer();
        //   }
        //   setIsSpeaking(false);
        //   return;
        // }
        
        // Reproduz chunk e atualiza estado
        console.log('‚ñ∂Ô∏è Reproduzindo √°udio (Barge-in desabilitado para POC)');
        if (audioPlaybackRef.current) {
          audioPlaybackRef.current.playAudioChunk(audioData).then(() => {
            if (audioPlaybackRef.current?.getIsPlaying()) {
              setIsSpeaking(true);
            }
          });
        }
      });

      // Callback para transcri√ß√£o do usu√°rio
      service.setOnTranscription((transcriptionText: string) => {
        console.log('üìù Transcri√ß√£o do usu√°rio recebida:', transcriptionText);
        setTranscription(transcriptionText);
      });

      // Callback para resposta do bot (texto)
      service.setOnText((text: string) => {
        console.log('üìù Texto recebido do servidor (bot_response):', text);
        
        // Usar refs para acessar valores atuais
        const currentTranscription = transcriptionRef.current;
        const currentMessages = messagesRef.current;
        
        // Se houver transcri√ß√£o anterior (pergunta do usu√°rio), adicionar como mensagem do usu√°rio primeiro
        if (currentTranscription && currentTranscription.trim() && 
            !currentMessages.some(m => m.text === currentTranscription && m.type === 'user')) {
          const userMessage: VoiceChatMessage = {
            id: `msg_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,
            text: currentTranscription,
            type: 'user',
            timestamp: new Date(),
          };
          setMessages((prev) => [...prev, userMessage]);
        }
        
        // Adicionar como mensagem do assistente
        const assistantMessage: VoiceChatMessage = {
          id: `msg_${Date.now() + 1}_${Math.random().toString(36).substr(2, 9)}`,
          text,
          type: 'assistant',
          timestamp: new Date(),
        };
        setMessages((prev) => [...prev, assistantMessage]);
        
        // Limpar transcri√ß√£o para pr√≥xima pergunta
        setTranscription('');
        setIsSpeaking(false);
      });

      await service.connect();
      serviceRef.current = service;
      setIsConnected(true);
      setError(null);
      console.log('‚úÖ WebSocket conectado com sucesso!');
    } catch (err) {
      console.error('‚ùå Erro ao conectar WebSocket:', err);
      setError(err as Error);
      setIsConnected(false);
    }
  }, [config]);

  /**
   * Handler: Usu√°rio come√ßa a falar
   */
  const handleSpeechStart = useCallback(() => {
    console.log('üó£Ô∏è Fala detectada pelo VAD');
    
    // Barge-in DESABILITADO para POC - n√£o para reprodu√ß√£o
    // No iOS, o VAD pode detectar falsos positivos (ru√≠do ambiente, feedback do √°udio)
    // Por isso desabilitamos completamente o Barge-in
    // TODO: Reativar Barge-in com melhorias para iOS quando necess√°rio
    
    // Apenas atualiza flag para logging, mas N√ÉO para reprodu√ß√£o
    setUserIsSpeaking(true);
    userIsSpeakingRef.current = true;
    
    // Barge-in desabilitado - comentado
    // if (audioPlaybackRef.current?.getIsPlaying()) {
    //   console.log('üõë Barge-in: parando reprodu√ß√£o porque usu√°rio come√ßou a falar');
    //   handleBargeIn();
    // }
  }, []);

  /**
   * Handler: Usu√°rio para de falar
   */
  const handleSpeechEnd = useCallback(() => {
    console.log('üîá Fim de fala detectado - desativando flag');
    setUserIsSpeaking(false);
    userIsSpeakingRef.current = false;
    // Aguarda um pouco antes de finalizar (evita cortes)
    setTimeout(() => {
      if (vadServiceRef.current?.getIsListening() && serviceRef.current) {
        serviceRef.current.endOfSpeech();
      }
    }, 500);
  }, []);

  /**
   * Inicia modo de voz (Real-Time)
   */
  const startVoiceMode = useCallback(async () => {
    console.log('üé§ ===== INICIANDO MODO DE VOZ =====');
    
    // N√ÉO esperar WebSocket - solicitar microfone primeiro
    // Tentar conectar WebSocket em paralelo (n√£o bloqueia)
    if (!serviceRef.current || !isConnected) {
      console.log('üì° Tentando conectar WebSocket em paralelo...');
      connect().catch((err) => {
        console.warn('‚ö†Ô∏è WebSocket n√£o conectou (continuando mesmo assim):', err);
      });
    }

    try {
      console.log('üéôÔ∏è Solicitando permiss√£o de microfone...');
      
      // Inicializar servi√ßos
      if (!vadServiceRef.current) {
        // Configura√ß√£o do VAD para melhor detec√ß√£o de sil√™ncio e redu√ß√£o de ru√≠do
        vadServiceRef.current = new VadService({
          silenceDuration: VAD_CONFIG.SILENCE_DURATION,
          vadThreshold: VAD_CONFIG.VAD_THRESHOLD, // undefined = usa padr√£o autom√°tico (0.025 desktop / 0.045 iOS)
          checkInterval: VAD_CONFIG.CHECK_INTERVAL,
        });
        console.log('‚úÖ VAD Service criado com configura√ß√£o:', {
          silenceDuration: VAD_CONFIG.SILENCE_DURATION + 'ms',
          vadThreshold: VAD_CONFIG.VAD_THRESHOLD || 'autom√°tico (0.025 desktop / 0.045 iOS)',
          checkInterval: VAD_CONFIG.CHECK_INTERVAL + 'ms',
        });
      }
      if (!audioCaptureRef.current) {
        audioCaptureRef.current = new AudioCaptureService();
        console.log('‚úÖ Audio Capture Service criado');
      }
      if (!audioPlaybackRef.current) {
        audioPlaybackRef.current = new AudioPlaybackService();
        await audioPlaybackRef.current.initialize();
        console.log('‚úÖ Audio Playback Service inicializado');
        
        // Callback quando playback parar
        audioPlaybackRef.current.onPlaybackStop(() => {
          setIsSpeaking(false);
        });
      }

      // Configurar callbacks do VAD
      vadServiceRef.current.onSpeechStart(() => {
        console.log('üó£Ô∏è Fala detectada!');
        handleSpeechStart();
      });
      vadServiceRef.current.onSpeechEnd(() => {
        console.log('üîá Fim de fala detectado');
        handleSpeechEnd();
      });

      // Iniciar VAD - isso vai solicitar permiss√£o de microfone
      console.log('üé§ Iniciando VAD (isso solicitar√° permiss√£o de microfone)...');
      await vadServiceRef.current.startListening();
      console.log('‚úÖ VAD iniciado, permiss√£o concedida');
      
      const vadStream = vadServiceRef.current.getAudioStream();
      
      if (!vadStream) {
        throw new Error('N√£o foi poss√≠vel obter stream de √°udio do VAD');
      }

      // Iniciar captura usando o stream do VAD
      console.log('üéôÔ∏è Iniciando captura de √°udio...');
      await audioCaptureRef.current.startCapture(vadStream, (chunk: ArrayBuffer) => {
        // Envia chunk apenas se estiver gravando e conectado
        if (serviceRef.current?.isConnected()) {
          console.log('üì§ Enviando chunk de √°udio:', chunk.byteLength, 'bytes');
          serviceRef.current.sendAudioChunk(chunk);
        } else {
          console.log('‚è∏Ô∏è Chunk capturado mas WebSocket n√£o conectado');
        }
      });
      
      console.log('‚úÖ Modo de voz iniciado com sucesso!');
      setIsRecording(true);
      setIsListening(true);
    } catch (err) {
      console.error('‚ùå Erro ao iniciar modo voz:', err);
      const errorMessage = err instanceof Error ? err.message : 'Erro desconhecido';
      setError(new Error(`Erro ao acessar microfone: ${errorMessage}. Verifique as permiss√µes.`));
      alert(`Erro ao acessar microfone: ${errorMessage}\n\nVerifique se voc√™ permitiu o acesso ao microfone nas configura√ß√µes do navegador.`);
    }
  }, [isConnected, connect, handleSpeechStart, handleSpeechEnd]);

  /**
   * Para modo de voz
   */
  const stopVoiceMode = useCallback(() => {
    if (audioCaptureRef.current) {
      audioCaptureRef.current.stopCapture();
    }
    if (vadServiceRef.current) {
      vadServiceRef.current.stopListening();
    }
    if (serviceRef.current) {
      serviceRef.current.endOfSpeech();
    }
    setIsRecording(false);
    setIsListening(false);
  }, []);

  /**
   * Desconecta
   */
  const disconnect = useCallback(() => {
    stopVoiceMode();
    
    if (serviceRef.current) {
      serviceRef.current.disconnect();
      serviceRef.current = null;
    }
    
    setIsConnected(false);
    setIsRecording(false);
    setIsListening(false);
    setIsSpeaking(false);
    setTranscription('');
  }, [stopVoiceMode]);

  /**
   * Envia mensagem de texto
   */
  const sendText = useCallback((text: string) => {
    if (serviceRef.current) {
      serviceRef.current.sendTextMessage(text);
    }
  }, []);

  // Cleanup ao desmontar
  useEffect(() => {
    return () => {
      disconnect();
    };
  }, [disconnect]);

  return {
    isConnected,
    isRecording,
    isListening,
    isSpeaking,
    transcription,
    messages,
    connect,
    disconnect,
    startVoiceMode,
    stopVoiceMode,
    sendText,
    error,
  };
}
